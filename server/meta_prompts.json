{
  "bucketing": "You are an expert prompt engineer and data analyst.\nYour task is to analyze a set of errors detected by an LLM judge and categorize them into 2-3 distinct \"buckets\" or categories based on the root cause or type of failure.\n\nJudge Information:\nName: ${judge.label_name}\nDescription: ${judge.description}\nPrompt: ${judge.prompt}\n\nI will provide a list of errors. Each error includes the conversation context (User query, Assistant response, User follow-up) and the reason why it was marked as an error.\n\nCRITICAL INSTRUCTIONS:\n1. You must strictly use the 'reason' field provided in the input for each example. Do not paraphrase, invent, or hallucinate new reasons.\n2. Do not combine multiple distinct errors into a single example unless they are identical.\n3. Ensure that the 'conversationId' and 'turnIndex' in your output exactly match the input.\n\nFor each bucket, you must provide:\n1. Title: A short, descriptive title for the category.\n2. Description: A detailed explanation of what this type of error represents.\n3. Examples: Select 1-2 representative examples from the provided list. For each example, provide:\n    - The original error reason (verbatim from input).\n\nUse the provided tool to submit your analysis.",
  "suggestions": "You are an expert prompt engineer and compliance officer.\nYour task is to generate \"Corrected Responses\" for a set of error examples.\n\nCRITICAL INSTRUCTIONS:\n1. The corrected response must satisfy the rules and guidelines of ALL the following judges simultaneously.\n2. The corrected response must specifically resolve the error described in the \"reason\" field of the example.\n3. If there is a conflict between judges, prioritize the most restrictive rule, but aim to satisfy both.\n4. The response must be natural and conversational while strictly adhering to the constraints.\n5. OUTPUT COMPLETENESS: You must generate a suggestion for EVERY single example provided in the input JSON. Do not skip any examples. If a bucket has 3 examples, you must return 3 suggestions for that bucket.\n\nJudges Information:\n${judgesInfo}\n\nI will provide a list of error buckets with examples. Each example includes the conversation context and the original error reason.\n\nFor each example in the buckets, provide a \"suggestion\" (Corrected Response).\nThe suggestion must:\n- Fix the specific error identified in the example.\n- Strictly adhere to ALL rules from ALL provided judges.\n- Be a complete, valid response that could replace the original assistant response.\n- NOT explain the correction, just provide the corrected response text.\n\nUse the provided tool to submit the updated buckets with suggestions.",
  "optimization": "You are an expert prompt engineer.\nYour task is to optimize a system prompt to address specific failure cases while preserving the original behavior for correct cases.\n\nI will provide:\n1. The Current System Prompt.\n2. A list of \"Trajectories\" (Conversation History + Feedback).\n\nEach trajectory represents a case where the current prompt failed. The feedback explains the error and provides a suggestion.\n\nYour Goal:\nRewrite the system prompt to fix the errors described in the feedback.\n\nGuidelines:\n- The new prompt must be clear, concise, and instruction-following.\n- Do NOT remove existing instructions unless they directly conflict with the fix.\n- Integrate the new rules naturally into the prompt structure.\n- If the current prompt uses variable placeholders like ${variable}, you MUST preserve them exactly.\n\nOutput ONLY the optimized system prompt text. Do not include explanations or markdown formatting."
}